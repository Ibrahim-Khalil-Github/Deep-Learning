{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7854872,
          "sourceType": "datasetVersion",
          "datasetId": 4606977
        },
        {
          "sourceId": 167336238,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Transformer",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ibrahim-Khalil-Github/Deep-Learning/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4606977%2F7854872%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240417%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240417T081315Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da327e722105a68daa864f9d179c9506da11cb2457d0e8ae9e9dc700973101d3bee85da449ed99883a999e62eea0fa97169ae2f4afa923d3acd250de8dff9dd99ea866edd70ba8d044c28c7c193cb4350aa15fac934d1a299f3402baa529daf08319360f6ae3c9d9165eafd17143a0920bd2b88cffcec03c599579ee89112f82ff97ac354f7dc46a455695501f70f43913385e95717f81ca0fd9f5b2ce41384f64ee9ca34e366afe63e10e9e1550e710e17128e853b7165815342e126adc462b8b5bd35c1b69111832fbda415e4a8090db09ccc3168ae1c48ef1dd74e27e0d8d8b36ad980b3d010301d38ffd00507b1ed6caa6e50b60e79255dce047b32ce02ba'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "7M1Ow4KoHofF"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data\n",
        "import math\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:26.827111Z",
          "iopub.execute_input": "2024-03-16T19:38:26.827365Z",
          "iopub.status.idle": "2024-03-16T19:38:30.608356Z",
          "shell.execute_reply.started": "2024-03-16T19:38:26.827343Z",
          "shell.execute_reply": "2024-03-16T19:38:30.607454Z"
        },
        "trusted": true,
        "id": "0bIsBkscHofJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "corpus_movie_conv = '/kaggle/input/dataset/movie_conversations.tsv'\n",
        "corpus_movie_lines = '/kaggle/input/dataset/movie_lines.tsv'\n",
        "max_len = 25"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:51:21.559402Z",
          "iopub.execute_input": "2024-03-16T18:51:21.560102Z",
          "iopub.status.idle": "2024-03-16T18:51:21.564059Z",
          "shell.execute_reply.started": "2024-03-16T18:51:21.56007Z",
          "shell.execute_reply": "2024-03-16T18:51:21.563175Z"
        },
        "id": "qvTrBPV_HofJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with open(corpus_movie_conv, 'r') as c:\n",
        "    conv = c.readlines()\n",
        "with open(corpus_movie_lines, 'r', encoding = 'utf8') as l:\n",
        "    lines = l.readlines()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:51:34.514785Z",
          "iopub.execute_input": "2024-03-16T18:51:34.515356Z",
          "iopub.status.idle": "2024-03-16T18:51:35.053336Z",
          "shell.execute_reply.started": "2024-03-16T18:51:34.515327Z",
          "shell.execute_reply": "2024-03-16T18:51:35.052533Z"
        },
        "id": "BMoQVTMwHofO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lines_dict = {}\n",
        "for line in lines:\n",
        "    objects = line.replace('\"', '').split('\\t')\n",
        "    lines_dict[objects[0]] = objects[-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:51:52.905067Z",
          "iopub.execute_input": "2024-03-16T18:51:52.905464Z",
          "iopub.status.idle": "2024-03-16T18:51:53.248183Z",
          "shell.execute_reply.started": "2024-03-16T18:51:52.905432Z",
          "shell.execute_reply": "2024-03-16T18:51:53.247085Z"
        },
        "id": "h_eR5MJTHofO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def remove_punc(string):\n",
        "    punctuations = '''!()-{}[];:'\"\\,<>./?@#$%^&*_~'''\n",
        "    no_punc = \"\"\n",
        "    for char in string:\n",
        "        if char not in punctuations:\n",
        "            no_punc = no_punc + char\n",
        "    return no_punc.lower()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:51:54.449907Z",
          "iopub.execute_input": "2024-03-16T18:51:54.450283Z",
          "iopub.status.idle": "2024-03-16T18:51:54.456013Z",
          "shell.execute_reply.started": "2024-03-16T18:51:54.450254Z",
          "shell.execute_reply": "2024-03-16T18:51:54.454932Z"
        },
        "id": "qRxrEyFyHofP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pairs = []\n",
        "\n",
        "for con in conv:\n",
        "    \n",
        "    ids = eval(con.replace(' ', ',').split('\\t')[-1])\n",
        "    \n",
        "    for i in range(len(ids)):\n",
        "        \n",
        "        if i == len(ids)-1:\n",
        "            break\n",
        "        \n",
        "        qa_pairs = []\n",
        "        \n",
        "        first = remove_punc(lines_dict[ids[i]].strip())\n",
        "        \n",
        "        second = remove_punc(lines_dict[ids[i+1]].strip())\n",
        "        \n",
        "        qa_pairs.append(first.split()[:max_len])\n",
        "        qa_pairs.append(second.split()[:max_len])\n",
        "        \n",
        "        pairs.append(qa_pairs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:51:55.760861Z",
          "iopub.execute_input": "2024-03-16T18:51:55.761496Z",
          "iopub.status.idle": "2024-03-16T18:52:03.584355Z",
          "shell.execute_reply.started": "2024-03-16T18:51:55.761456Z",
          "shell.execute_reply": "2024-03-16T18:52:03.583527Z"
        },
        "id": "oGt_sagVHofP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "word_freq = Counter()\n",
        "for pair in pairs:\n",
        "    word_freq.update(pair[0])\n",
        "    word_freq.update(pair[1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:52:08.9452Z",
          "iopub.execute_input": "2024-03-16T18:52:08.94604Z",
          "iopub.status.idle": "2024-03-16T18:52:10.022461Z",
          "shell.execute_reply.started": "2024-03-16T18:52:08.946004Z",
          "shell.execute_reply": "2024-03-16T18:52:10.02142Z"
        },
        "id": "GcMA9elSHofP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "min_word_freq = 5\n",
        "\n",
        "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
        "word_map = {k: v+1 for v, k in enumerate(words)}\n",
        "\n",
        "word_map['<unk>'] = len(word_map) + 1\n",
        "word_map['<start>'] = len(word_map) + 1\n",
        "word_map['<end>'] = len(word_map) + 1\n",
        "word_map['<pad>'] = 0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:52:11.7943Z",
          "iopub.execute_input": "2024-03-16T18:52:11.794643Z",
          "iopub.status.idle": "2024-03-16T18:52:11.829648Z",
          "shell.execute_reply.started": "2024-03-16T18:52:11.794616Z",
          "shell.execute_reply": "2024-03-16T18:52:11.828887Z"
        },
        "id": "UkFio7msHofQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with open('/kaggle/working/WORDMAP_corpus.json', 'w') as j:\n",
        "    json.dump(word_map, j)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:52:18.229417Z",
          "iopub.execute_input": "2024-03-16T18:52:18.230089Z",
          "iopub.status.idle": "2024-03-16T18:52:18.273615Z",
          "shell.execute_reply.started": "2024-03-16T18:52:18.230051Z",
          "shell.execute_reply": "2024-03-16T18:52:18.272615Z"
        },
        "id": "PXncDcyIHofQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def encode_question(words, word_map):\n",
        "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']]*(max_len-len(words))\n",
        "    return enc_c"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:52:20.058318Z",
          "iopub.execute_input": "2024-03-16T18:52:20.058686Z",
          "iopub.status.idle": "2024-03-16T18:52:20.063854Z",
          "shell.execute_reply.started": "2024-03-16T18:52:20.058655Z",
          "shell.execute_reply": "2024-03-16T18:52:20.062801Z"
        },
        "id": "6TT5bfPzHofQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def encode_reply(words, word_map):\n",
        "    enc_c =[word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']] + [word_map['<pad>']]*(max_len-len(words))\n",
        "    return enc_c"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:52:21.405632Z",
          "iopub.execute_input": "2024-03-16T18:52:21.405984Z",
          "iopub.status.idle": "2024-03-16T18:52:21.414486Z",
          "shell.execute_reply.started": "2024-03-16T18:52:21.405956Z",
          "shell.execute_reply": "2024-03-16T18:52:21.413285Z"
        },
        "id": "J9EwFXRAHofQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pairs_encoded = []\n",
        "\n",
        "for pair in pairs:\n",
        "    ques = encode_question(pair[0], word_map)\n",
        "    ans = encode_reply(pair[1], word_map)\n",
        "    pairs_encoded.append([ques, ans])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:52:22.617819Z",
          "iopub.execute_input": "2024-03-16T18:52:22.618394Z",
          "iopub.status.idle": "2024-03-16T18:52:25.398186Z",
          "shell.execute_reply.started": "2024-03-16T18:52:22.618363Z",
          "shell.execute_reply": "2024-03-16T18:52:25.397159Z"
        },
        "id": "4RE8ocfEHofQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "with open('/kaggle/working/pairs_encoded.json', 'w') as w:\n",
        "    json.dump(pairs_encoded, w)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T18:52:30.61754Z",
          "iopub.execute_input": "2024-03-16T18:52:30.618237Z",
          "iopub.status.idle": "2024-03-16T18:52:42.045694Z",
          "shell.execute_reply.started": "2024-03-16T18:52:30.618199Z",
          "shell.execute_reply": "2024-03-16T18:52:42.044681Z"
        },
        "id": "5IF9VZSWHofR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pairs = json.load(open('/kaggle/input/transformer/pairs_encoded.json', 'r'))\n",
        "        self.dataset_size = len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        question = torch.LongTensor(self.pairs[i][0])\n",
        "        reply = torch.LongTensor(self.pairs[i][1])\n",
        "\n",
        "        return question, reply\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:30.609853Z",
          "iopub.execute_input": "2024-03-16T19:38:30.610222Z",
          "iopub.status.idle": "2024-03-16T19:38:30.616345Z",
          "shell.execute_reply.started": "2024-03-16T19:38:30.610197Z",
          "shell.execute_reply": "2024-03-16T19:38:30.615442Z"
        },
        "trusted": true,
        "id": "0jSsBJQoHofR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(Dataset(),\n",
        "                                           shuffle = True,\n",
        "                                           batch_size = 100,\n",
        "                                           pin_memory = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:30.617507Z",
          "iopub.execute_input": "2024-03-16T19:38:30.617779Z",
          "iopub.status.idle": "2024-03-16T19:38:33.612219Z",
          "shell.execute_reply.started": "2024-03-16T19:38:30.617756Z",
          "shell.execute_reply": "2024-03-16T19:38:33.611225Z"
        },
        "trusted": true,
        "id": "EwDrJS90HofR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(question, reply_input, reply_target):\n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.tril(torch.ones(size, size)).type(dtype = torch.uint8)\n",
        "        return mask\n",
        "\n",
        "    question_mask = question!=0 #cuda\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "    reply_input_mask = reply_input!=0 #cuda\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.shape[-1]).type_as(reply_input_mask.data)\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)\n",
        "    reply_target_mask = reply_target!=0\n",
        "\n",
        "    return question_mask, reply_input_mask, reply_target_mask"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.614695Z",
          "iopub.execute_input": "2024-03-16T19:38:33.615179Z",
          "iopub.status.idle": "2024-03-16T19:38:33.622087Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.615144Z",
          "shell.execute_reply": "2024-03-16T19:38:33.620971Z"
        },
        "trusted": true,
        "id": "rZvFtLQsHofR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, max_len = 50):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positional_encoding(max_len, self.d_model)\n",
        "\n",
        "    def create_positional_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).cuda()\n",
        "\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos/(1000**(2*i/d_model)))\n",
        "                pe[pos, i+1] = math.cos(pos/(1000**(2*(i+1)/d_model)))\n",
        "\n",
        "        return pe\n",
        "\n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        embedding += self.pe[:embedding.size(1), :]\n",
        "        embedding = self.dropout(embedding)\n",
        "\n",
        "        return embedding"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.623062Z",
          "iopub.execute_input": "2024-03-16T19:38:33.623371Z",
          "iopub.status.idle": "2024-03-16T19:38:33.644113Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.623348Z",
          "shell.execute_reply": "2024-03-16T19:38:33.643226Z"
        },
        "trusted": true,
        "id": "c0IIzqcYHofR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, heads, d_model):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads ==0\n",
        "\n",
        "        self.d_k = d_model//heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)\n",
        "        value = self.value(value)\n",
        "\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        key = key.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "        value = value.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n",
        "\n",
        "        scores = torch.matmul(query, key.permute(0, 1, 3, 2))/math.sqrt(query.shape[-1])\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        weights = F.softmax(scores, dim = -1)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        context = torch.matmul(weights, value)\n",
        "        context = context.permute(0, 2, 1, 3).reshape(context.shape[0], -1, self.heads*self.d_k)\n",
        "\n",
        "        interacted = self.concat(context)\n",
        "\n",
        "        return interacted"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.645473Z",
          "iopub.execute_input": "2024-03-16T19:38:33.645851Z",
          "iopub.status.idle": "2024-03-16T19:38:33.657715Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.645811Z",
          "shell.execute_reply": "2024-03-16T19:38:33.656923Z"
        },
        "trusted": true,
        "id": "JOu0pXZnHofR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.dropout(F.relu(self.fc1(x)))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.65889Z",
          "iopub.execute_input": "2024-03-16T19:38:33.659188Z",
          "iopub.status.idle": "2024-03-16T19:38:33.67067Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.659164Z",
          "shell.execute_reply": "2024-03-16T19:38:33.669952Z"
        },
        "trusted": true,
        "id": "HZLOCvdBHofS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted+embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(interacted+feed_forward_out)\n",
        "\n",
        "        return encoded"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.671699Z",
          "iopub.execute_input": "2024-03-16T19:38:33.67199Z",
          "iopub.status.idle": "2024-03-16T19:38:33.680723Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.671959Z",
          "shell.execute_reply": "2024-03-16T19:38:33.679939Z"
        },
        "trusted": true,
        "id": "NqtBzPzAHofS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(query + interacted)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "\n",
        "        return decoded"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.681861Z",
          "iopub.execute_input": "2024-03-16T19:38:33.682483Z",
          "iopub.status.idle": "2024-03-16T19:38:33.693826Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.682451Z",
          "shell.execute_reply": "2024-03-16T19:38:33.693076Z"
        },
        "trusted": true,
        "id": "A3ab_FdHHofS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads, num_layers, word_map):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = len(word_map)\n",
        "\n",
        "        self.embed = Embeddings(self.vocab_size, d_model)\n",
        "\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
        "\n",
        "    def encode(self, src_words, src_mask):\n",
        "\n",
        "        src_embeddings = self.embed(src_words)\n",
        "\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "\n",
        "        return src_embeddings\n",
        "\n",
        "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "\n",
        "        tgt_embeddings = self.embed(target_words)\n",
        "\n",
        "        for layer in self.decoder:\n",
        "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "\n",
        "        return tgt_embeddings\n",
        "\n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "\n",
        "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.696459Z",
          "iopub.execute_input": "2024-03-16T19:38:33.6968Z",
          "iopub.status.idle": "2024-03-16T19:38:33.706336Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.696776Z",
          "shell.execute_reply": "2024-03-16T19:38:33.705327Z"
        },
        "trusted": true,
        "id": "_AZTa5cYHofS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamWarmup:\n",
        "\n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "\n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "\n",
        "    def get_lr(self):\n",
        "\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "\n",
        "    def step(self):\n",
        "\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.70737Z",
          "iopub.execute_input": "2024-03-16T19:38:33.707625Z",
          "iopub.status.idle": "2024-03-16T19:38:33.719149Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.707603Z",
          "shell.execute_reply": "2024-03-16T19:38:33.718356Z"
        },
        "trusted": true,
        "id": "1Si9DGT3HofS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossWithLS(nn.Module):\n",
        "\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "\n",
        "        self.criterion = nn.KLDivLoss(size_average = False, reduce = False)\n",
        "        self.size = size\n",
        "        self.smooth = smooth\n",
        "        self.confidence = 1.0 - smooth\n",
        "\n",
        "    def forward(self, prediction, target, mask):\n",
        "\n",
        "        prediction = prediction.view(-1, prediction.size(-1))\n",
        "        target = target.contiguous().view(-1)\n",
        "\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)\n",
        "\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth/(self.size - 1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "\n",
        "        loss = self.criterion(prediction, labels)\n",
        "\n",
        "        loss = (loss.sum(1)*mask).sum() / mask.sum()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.720289Z",
          "iopub.execute_input": "2024-03-16T19:38:33.720623Z",
          "iopub.status.idle": "2024-03-16T19:38:33.73239Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.720593Z",
          "shell.execute_reply": "2024-03-16T19:38:33.731628Z"
        },
        "trusted": true,
        "id": "RGoIRP6bHofT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 6\n",
        "epochs = 10\n",
        "model_size = d_model\n",
        "warmup_steps = 4000\n",
        "\n",
        "\n",
        "with open('/kaggle/input/transformer/WORDMAP_corpus.json', 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "\n",
        "size = len(word_map)\n",
        "smooth = 0.1\n",
        "\n",
        "criterion = LossWithLS(size, smooth).cuda()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.733376Z",
          "iopub.execute_input": "2024-03-16T19:38:33.73363Z",
          "iopub.status.idle": "2024-03-16T19:38:33.759398Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.733608Z",
          "shell.execute_reply": "2024-03-16T19:38:33.758595Z"
        },
        "trusted": true,
        "id": "bohxcENpHofT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformer = Transformer(d_model, heads, num_layers, word_map).cuda()\n",
        "\n",
        "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr = 0, betas = (0.9, 0.98), eps = 1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size, warmup_steps, adam_optimizer )"
      ],
      "metadata": {
        "id": "FIZOUkVsHofT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, transformer, criterion, epoch):\n",
        "\n",
        "    transformer.train()\n",
        "\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for i, (question, reply) in enumerate(train_loader):\n",
        "\n",
        "        samples = question.shape[0]\n",
        "\n",
        "        question = question.cuda()\n",
        "        reply = reply.cuda()\n",
        "\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
        "\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "\n",
        "        loss = criterion(out, reply_target, reply_target_mask)\n",
        "\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "\n",
        "        sum_loss += loss.item() * samples\n",
        "        count += samples\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch: {}, Iterations: {}/{}, Loss: {:.3f}'.format(epoch, i, len(train_loader), sum_loss/count))\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.760506Z",
          "iopub.execute_input": "2024-03-16T19:38:33.760767Z",
          "iopub.status.idle": "2024-03-16T19:38:33.768219Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.760744Z",
          "shell.execute_reply": "2024-03-16T19:38:33.767382Z"
        },
        "trusted": true,
        "id": "rOqBY3RDHofT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
        "\n",
        "    rev_word_map = {v: k for k, v in word_map.items()}\n",
        "\n",
        "    transformer.eval()\n",
        "\n",
        "    start_token = word_map['<start>']\n",
        "\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "\n",
        "    words = torch.LongTensor([[start_token]]).cuda()\n",
        "\n",
        "    for step in range(max_len - 1):\n",
        "\n",
        "        size = words.shape[0]\n",
        "\n",
        "        target_mask = torch.tril(torch.ones(size, size)).type(dtype = torch.uint8).cuda()\n",
        "        target_mask = target_mask.unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "\n",
        "        prediction = transformer.logit(decoded[:, -1])\n",
        "        _, next_word = torch.max(prediction, dim = -1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        if next_word == word_map['<end>']:\n",
        "            break\n",
        "        words = torch.cat([words, torch.LongTensor([[next_word]]).cuda()], dim = -1)\n",
        "\n",
        "    words = words.squeeze(0)\n",
        "    words = words.tolist()\n",
        "\n",
        "    sen_idx = [w for w in words if w != word_map['<start>']]\n",
        "    sentence = \" \".join([rev_word_map[w] for w in sen_idx])\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.769153Z",
          "iopub.execute_input": "2024-03-16T19:38:33.769423Z",
          "iopub.status.idle": "2024-03-16T19:38:33.78159Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.769392Z",
          "shell.execute_reply": "2024-03-16T19:38:33.7808Z"
        },
        "trusted": true,
        "id": "KidZzhYwHofT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('/kaggle/input/transformer/checkpoint9.pth')\n",
        "transformer = checkpoint['transformer']\n",
        "transformer_optimizer = checkpoint['transformer_optimizer']\n",
        "epoch = checkpoint['epoch']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:33.782752Z",
          "iopub.execute_input": "2024-03-16T19:38:33.783378Z",
          "iopub.status.idle": "2024-03-16T19:38:40.35492Z",
          "shell.execute_reply.started": "2024-03-16T19:38:33.783347Z",
          "shell.execute_reply": "2024-03-16T19:38:40.353958Z"
        },
        "trusted": true,
        "id": "7WxVJwf7HofT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epoch+1, epochs+1+epoch):\n",
        "    train(train_loader, transformer, criterion, epoch)\n",
        "    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
        "    torch.save(state, 'checkpoint'+str(epoch)+'.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-16T19:38:40.356217Z",
          "iopub.execute_input": "2024-03-16T19:38:40.356663Z"
        },
        "trusted": true,
        "id": "9Ty6HpuEHofT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    question = 'nothing'\n",
        "    max_len = 21\n",
        "    enc_ques = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
        "    question = torch.tensor(enc_ques).cuda().unsqueeze(0)\n",
        "    question_mask = (question !=0).cuda().unsqueeze(1).unsqueeze(1)\n",
        "    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
        "    print(sentence)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HPai1Y8qHofT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bd7ccNQvHofU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}